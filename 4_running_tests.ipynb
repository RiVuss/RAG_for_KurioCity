{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user1\\anaconda3\\envs\\RAGLLMs\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from rag_objects.RAG_system_A import RAGSystemA\n",
    "from rag_objects.RAG_system_B import RAGSystemB\n",
    "from rag_objects.RAG_system_C import RAGSystemC\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "#print(torch.version.cuda)  \n",
    "#\"i'm sorry, but none of the provided locations are palaces in the hague.  the query is too specific for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing pipeline\n",
    "## Test definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_cases = [\n",
    "    {\"user_query\": \"skyscrapers in the Hague\", \"golden_route\": [\"Hoftoren\", \"Het Strijkijzer\", \"De Kroon (woontoren)\",\"Castalia (gebouw)\",\"Zurichtoren\"]},\n",
    "    {\"user_query\": \"What should a museum nerd see in Haarlem\", \"golden_route\": [\"Frans Hals Museum\", \"Verwey Museum Haarlem\", \"Teylers Museum\",\"Het Dolhuys\", \"Archeologisch Museum Haarlem\"]},\n",
    "    {\"user_query\": \"I like metro systems. Which stations are interesting in Amsterdam?\", \"golden_route\": [\"Nieuwmarkt (metrostation)\", \"Rokin (metrostation)\", \"Vijzelgracht (metrostation)\",\"Weesperplein (metrostation)\", \"Noorderpark (metrostation)\"]},\n",
    "    {\"user_query\": \"brutalist architecture Amsterdam\", \"golden_route\": [\"Hoofdgebouw Vrije Universiteit\", \"Leeuwenburg (Amsterdam)\", \"Louwesweg\",\"Kraanspoor\", \"Weteringschans 26-28\"]},\n",
    "    {\"user_query\": \"I want to see the most famous bridges of Amsterdam\", \"golden_route\": [\"Oudekerksbrug\", \"Blauwbrug\", \"Aluminiumbrug\",\"Torensluis\", \"Sint Antoniessluishoogwaterkering\"]},\n",
    "    {\"user_query\": \"the palaces of the Hague\", \"golden_route\": [\"Paleis Kneuterdijk\", \"Paleis Noordeinde\", \"Mauritshuis\",\"Paleis Huis ten Bosch\", \"Vredespaleis\"]},\n",
    "    {\"user_query\": \"What to see in Amsterdam to learn about the jewish heritage\", \"golden_route\": [\"Jodenbuurt (Amsterdam)\", \"Anne Frank Huis\", \"Nationaal Holocaustmuseum\",\"Holocaust Namenmonument\", \"Portugees-IsraÃ«lietische Synagoge\"]},\n",
    "    {\"user_query\": \"What should Rembrandt lover see in Leiden?\", \"golden_route\": [\"Latijnse school (Leiden)\", \"Rembrandtbrug\", \"Pieterskerk (Leiden)\",\"Langebrug (Leiden)\", \"Museum De Lakenhal\"]}\n",
    "]\n",
    "\n",
    "# test_cases = [\n",
    "#     {\"user_query\": \"skyscrapers in the Hague\", \"golden_route\": [\"Hoftoren\", \"Het Strijkijzer\", \"De Kroon (woontoren)\",\"Castalia (gebouw)\",\"Zurichtoren\"]},\n",
    "#     {\"user_query\": \"What should a museum nerd see in Haarlem\", \"golden_route\": [\"Frans Hals Museum\", \"Verwey Museum Haarlem\", \"Teylers Museum\",\"Het Dolhuys\", \"Archeologisch Museum Haarlem\"]}\n",
    "# ]\n",
    "\n",
    "embedding_models = {\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\": \"embeddings/all-MiniLM-L6-v2_faiss_index.index\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\": \"embeddings/all-mpnet-base-v2_faiss_index.index\",\n",
    "    \"NovaSearch/stella_en_1.5B_v5\" : \"embeddings/stella_en_1_5B_v5_embeddings_faiss_index.index\"\n",
    "}\n",
    "\n",
    "gemini_token_file = \"API_tokens/gemini.txt\"\n",
    "\n",
    "def evaluate_rag_system(rag_system, test_cases, rag_system_name, embedding_model_name, top_k=8):\n",
    "    \"\"\"\n",
    "    Evaluate a RAG system by running multiple test cases and computing retrieval metrics.\n",
    "\n",
    "    Args:\n",
    "        rag_system (RAGSystemA): The RAG system to test.\n",
    "        test_cases (list of dict): Each dict should contain:\n",
    "            - 'user_query': The query string.\n",
    "            - 'golden_route': List of ideal location titles.\n",
    "        rag_system_name (str): Name of the RAG system being tested.\n",
    "        embedding_model_name (str): Name of the embedding model used.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing results for all test cases.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        user_query = test_case[\"user_query\"]\n",
    "        golden_route = test_case[\"golden_route\"]\n",
    "        \n",
    "        # Run query\n",
    "        result = rag_system.query(user_query=user_query, top_k=top_k)\n",
    "        \n",
    "        # Extract retrieved titles\n",
    "        retrieved_titles = [loc[\"title\"] for loc in result.get(\"locations\", [])]\n",
    "        \n",
    "        # Compute golden route metrics\n",
    "        retrieved_set = set(retrieved_titles)\n",
    "        golden_set = set(golden_route)\n",
    "        common = retrieved_set.intersection(golden_set)\n",
    "        precision = len(common) / len(retrieved_titles) if retrieved_titles else 0\n",
    "        recall = len(common) / len(golden_set) if golden_set else 0\n",
    "        num_retrieved_from_golden = len(common)\n",
    "        \n",
    "        # Store results dynamically including all dictionary keys\n",
    "        result_entry = {\n",
    "            \"rag_system\": rag_system_name,\n",
    "            \"embedding_model\": embedding_model_name,\n",
    "            \"query\": user_query,\n",
    "            \"retrieved_titles\": retrieved_titles,\n",
    "            \"golden_route\": golden_route,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"num_retrieved_from_golden\": num_retrieved_from_golden,\n",
    "        }\n",
    "        \n",
    "        # Add all key-value pairs dynamically\n",
    "        for key, value in result.items():\n",
    "            if key not in result_entry:\n",
    "                result_entry[key] = value\n",
    "        \n",
    "        results.append(result_entry)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running tests\n",
    "### RAG A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "RAGSystemA initialized and ready.\n",
      "Evaluating with embedding model: sentence-transformers/all-mpnet-base-v2\n",
      "RAGSystemA initialized and ready.\n",
      "Evaluating with embedding model: NovaSearch/stella_en_1.5B_v5\n",
      "RAGSystemA initialized and ready.\n",
      "Evaluation completed. Results saved to 'test_results/A_rag_results.csv'\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for model_name, index_path in embedding_models.items():\n",
    "    print(f\"Evaluating with embedding model: {model_name}\")\n",
    "    \n",
    "    rag_system = RAGSystemA(index_path=index_path, embedding_model_name=model_name, gemini_token_file=gemini_token_file)\n",
    "    \n",
    "    # Run evaluation and collect results\n",
    "    df_results = evaluate_rag_system(rag_system, test_cases, \"RAGSystemA\", model_name, top_k=8)\n",
    "    all_results.append(df_results)\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "final_results_df = pd.concat(all_results, ignore_index=True)\n",
    "final_results_df.to_csv(\"test_results/A_rag_results.csv\", index=False)\n",
    "\n",
    "print(\"Evaluation completed. Results saved to 'test_results/A_rag_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "RAGSystemB initialized and ready.\n",
      "Evaluating with embedding model: sentence-transformers/all-mpnet-base-v2\n",
      "RAGSystemB initialized and ready.\n",
      "Evaluating with embedding model: NovaSearch/stella_en_1.5B_v5\n",
      "RAGSystemB initialized and ready.\n",
      "Evaluation completed. Results saved to 'test_results/B_rag_results.csv'\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for model_name, index_path in embedding_models.items():\n",
    "    print(f\"Evaluating with embedding model: {model_name}\")\n",
    "    \n",
    "    rag_system = RAGSystemB(index_path=index_path, embedding_model_name=model_name, gemini_token_file=gemini_token_file)\n",
    "    \n",
    "    # Run evaluation and collect results\n",
    "    df_results = evaluate_rag_system(rag_system, test_cases, \"RAGSystemB\", model_name,top_k=16)\n",
    "    all_results.append(df_results)\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "final_results_df = pd.concat(all_results, ignore_index=True)\n",
    "final_results_df.to_csv(\"test_results/B_rag_results.csv\", index=False)\n",
    "\n",
    "print(\"Evaluation completed. Results saved to 'test_results/B_rag_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "RAGSystemC initialized and ready.\n",
      "Evaluating with embedding model: sentence-transformers/all-mpnet-base-v2\n",
      "RAGSystemC initialized and ready.\n",
      "Evaluating with embedding model: NovaSearch/stella_en_1.5B_v5\n",
      "RAGSystemC initialized and ready.\n",
      "Evaluation completed. Results saved to 'test_results/C_rag_results.csv'\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for model_name, index_path in embedding_models.items():\n",
    "    print(f\"Evaluating with embedding model: {model_name}\")\n",
    "    \n",
    "    rag_system = RAGSystemC(index_path=index_path, embedding_model_name=model_name, gemini_token_file=gemini_token_file)\n",
    "    \n",
    "    # Run evaluation and collect results\n",
    "    df_results = evaluate_rag_system(rag_system, test_cases, \"RAGSystemC\", model_name,top_k=8)\n",
    "    all_results.append(df_results)\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "final_results_df = pd.concat(all_results, ignore_index=True)\n",
    "final_results_df.to_csv(\"test_results/C_rag_results.csv\", index=False)\n",
    "\n",
    "print(\"Evaluation completed. Results saved to 'test_results/C_rag_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results CSV\n",
    "csv_path = \"test_results/B_rag_results.csv\"  # Change this if needed\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Initialize rating columns\n",
    "df[\"irrelevant_wrong_area\"] = [[] for _ in range(len(df))]\n",
    "df[\"relevant_wrong_area\"] = [[] for _ in range(len(df))]\n",
    "df[\"irrelevant\"] = [[] for _ in range(len(df))]\n",
    "df[\"relevant\"] = [[] for _ in range(len(df))]\n",
    "df[\"good\"] = [[] for _ in range(len(df))]\n",
    "\n",
    "# Initialize count columns\n",
    "df[\"count_irrelevant_wrong_area\"] = 0\n",
    "df[\"count_relevant_wrong_area\"] = 0\n",
    "df[\"count_irrelevant\"] = 0\n",
    "df[\"count_relevant\"] = 0\n",
    "df[\"count_good\"] = 0\n",
    "\n",
    "def display_map(latitude, longitude, title):\n",
    "    \"\"\" Display a full-sized Folium map centered at the given coordinates. \"\"\"\n",
    "    m = folium.Map(location=[latitude, longitude], zoom_start=12, control_scale=True, height=400)\n",
    "    folium.Marker([latitude, longitude], popup=title, tooltip=title).add_to(m)\n",
    "    display(m)\n",
    "\n",
    "def rate_location(row_idx, locations):\n",
    "    \"\"\" Manually input a rating for each retrieved location. \"\"\"\n",
    "    ratings = {\n",
    "        1: \"Irrelevant, Wrong Area\",\n",
    "        2: \"Relevant, Wrong Area\",\n",
    "        3: \"Irrelevant\",\n",
    "        4: \"Relevant\",\n",
    "        5: \"Good\"\n",
    "    }\n",
    "    \n",
    "    location_ratings = {1: [], 2: [], 3: [], 4: [], 5: []}\n",
    "    \n",
    "    for location in locations:\n",
    "        clear_output(wait=True)\n",
    "        title = location[\"title\"]\n",
    "        description = location[\"generated_text\"]\n",
    "        latitude = location[\"latitude\"]\n",
    "        longitude = location[\"longitude\"]\n",
    "\n",
    "        print(f\"User Query: {df.loc[row_idx, 'query']}\")\n",
    "        print(f\"\\nLocation: {title}\")\n",
    "        print(f\"Description: {description[:1500]}...\\n\")\n",
    "\n",
    "        # Print rating options before displaying the map\n",
    "        print(\"\\nRating Guide:\")\n",
    "        for key, value in ratings.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "        display_map(latitude, longitude, title)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                rating = int(input(f\"Enter rating (1-5) for '{title}': \").strip())\n",
    "                if rating in ratings:\n",
    "                    location_ratings[rating].append(title)\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Invalid input. Please enter a number between 1 and 5.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a valid number between 1 and 5.\")\n",
    "\n",
    "    return location_ratings\n",
    "\n",
    "def manual_rating_pipeline():\n",
    "    \"\"\" Run through all rows and manually rate retrieved locations. \"\"\"\n",
    "    for row_idx in range(len(df)):\n",
    "        locations = eval(df.loc[row_idx, \"locations\"])  # Convert string to list\n",
    "        ratings = rate_location(row_idx, locations)\n",
    "\n",
    "        df.at[row_idx, \"irrelevant_wrong_area\"] = ratings[1]\n",
    "        df.at[row_idx, \"relevant_wrong_area\"] = ratings[2]\n",
    "        df.at[row_idx, \"irrelevant\"] = ratings[3]\n",
    "        df.at[row_idx, \"relevant\"] = ratings[4]\n",
    "        df.at[row_idx, \"good\"] = ratings[5]\n",
    "\n",
    "        # Store only the counts per category\n",
    "        df.at[row_idx, \"count_irrelevant_wrong_area\"] = len(ratings[1])\n",
    "        df.at[row_idx, \"count_relevant_wrong_area\"] = len(ratings[2])\n",
    "        df.at[row_idx, \"count_irrelevant\"] = len(ratings[3])\n",
    "        df.at[row_idx, \"count_relevant\"] = len(ratings[4])\n",
    "        df.at[row_idx, \"count_good\"] = len(ratings[5])\n",
    "\n",
    "        clear_output()\n",
    "        print(\"Summary of Ratings for Current Query:\")\n",
    "        print(f\"User Query: {df.at[row_idx, 'query']}\")\n",
    "\n",
    "        print(f\"Irrelevant, Wrong Area: {df.at[row_idx, 'count_irrelevant_wrong_area']}\")\n",
    "        print(f\"Relevant, Wrong Area: {df.at[row_idx, 'count_relevant_wrong_area']}\")\n",
    "        print(f\"Irrelevant: {df.at[row_idx, 'count_irrelevant']}\")\n",
    "        print(f\"Relevant: {df.at[row_idx, 'count_relevant']}\")\n",
    "        print(f\"Good: {df.at[row_idx, 'count_good']}\")\n",
    "\n",
    "        df.to_csv(\"test_results/B_rated_rag_results.csv\", index=False)          #CHANGE\n",
    "        input(\"Press Enter to proceed to the next query...\")\n",
    "\n",
    "    df.to_csv(\"test_results/B_rated_rag_results.csv\", index=False)              #CHANGE\n",
    "    print(\"All ratings saved to 'test_results/B_rated_rag_results.csv'.\")       #CHANGE\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Ratings for Current Query:\n",
      "User Query: What should Rembrandt lover see in Leiden?\n",
      "Irrelevant, Wrong Area: 0\n",
      "Relevant, Wrong Area: 0\n",
      "Irrelevant: 0\n",
      "Relevant: 0\n",
      "Good: 0\n",
      "All ratings saved to 'test_results/B_rated_rag_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "rated_df = manual_rating_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGLLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
